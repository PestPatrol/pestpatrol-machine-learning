# -*- coding: utf-8 -*-
"""modelRiceLeaf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Csoovq7sFhuG7J2nOuTYlemRZGHZwD6r
"""

!pip install tensorflowjs

!pip install TensorFlow==2.15.0
!pip install tensorflow-decision-forests==1.8.1

import tensorflowjs as tfjs
print(tfjs.__version__)

# i = []
# while True:
#   i.append(i)

"""import package"""

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import cv2
import numpy as np
import pickle
import pandas as pd
import glob

"""load dataset"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path ke file ZIP di Google Drive
zip_path = '/content/drive/My Drive/combined_dataset.zip'

# Direktori tujuan untuk ekstraksi
extract_path = '/content/combined_dataset/'

# Ekstrak file ZIP
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List files di direktori ekstraksi untuk memastikan ekstraksi berhasil
os.listdir(extract_path)

BATCH_SIZE = 32
IMAGE_SIZE = 224

dataset = image_dataset_from_directory(
    "/content/combined_dataset",
    seed=123,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

class_names = dataset.class_names
class_names

"""image preprocessing"""

plt.figure(figsize=(15, 10))
for img, label in dataset.take(1):
    img_np = img.numpy()  # Convert tf.Tensor to numpy array

    for i in range(12):
        ax = plt.subplot(4, 6, i + 1)

        img_np_i = img_np[i].astype("uint8")
        blur_img = cv2.GaussianBlur(img_np_i, (5, 5), 0)
        hsv_img = cv2.cvtColor(blur_img, cv2.COLOR_RGB2HSV)

        lower_green = (25, 40, 40)
        upper_green = (100, 255, 255)

        mask = cv2.inRange(hsv_img, lower_green, upper_green)

        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

        green_objects = cv2.bitwise_and(img_np_i, img_np_i, mask=mask)
        green_objects_rgb = cv2.cvtColor(green_objects, cv2.COLOR_BGR2RGB)
        green_objects_rgb = cv2.convertScaleAbs(green_objects_rgb, alpha=1.5, beta=50)

        plt.imshow(green_objects_rgb)
        plt.title(class_names[label[i].numpy()])  # Convert label to numpy scalar
        plt.axis("off")

plt.show()

# Function to convert labels to one-hot encoding
def one_hot_encode(image, label):
    label = tf.one_hot(label, depth=len(class_names))
    return image, label

# Apply one-hot encoding to the dataset
dataset = dataset.map(one_hot_encode)

# Displaying sample images after preprocessing and showing one-hot encoded labels
plt.figure(figsize=(10, 10))
for images, labels in dataset.take(1):
    for i in range(12):
        ax = plt.subplot(3, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        label_index = np.argmax(labels[i].numpy())
        plt.title(class_names[label_index])
        plt.axis("off")
        print(f"Image {i+1} one-hot encoded label: {labels[i].numpy()}")
plt.show()

"""split data"""

# Function to partition the dataset
def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
    assert (train_split + test_split + val_split) == 1

    ds_size = len(ds)

    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)

    train_ds = ds.take(train_size)
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)

    return train_ds, val_ds, test_ds

# Partition the dataset
train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

"""data augmentasi"""

# Data augmentation function
def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)
    image = tf.image.random_brightness(image, max_delta=0.1)
    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)
    image = tf.image.random_saturation(image, lower=0.9, upper=1.1)
    image = tf.image.random_hue(image, max_delta=0.1)
    return image, label

# Apply data augmentation to the training dataset
train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# Function to get the class index from one-hot encoded labels
def get_class_index(one_hot_encoded_label):
    return tf.argmax(one_hot_encoded_label).numpy()

# Display some augmented images
plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        class_index = get_class_index(labels[i])
        plt.title(class_names[class_index])
        plt.axis("off")
plt.show()

"""model"""

import keras
from keras.layers import Dropout
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.applications.efficientnet_v2 import EfficientNetV2M

# Load the pre-trained ResNet50 model without the top layer
base_model = EfficientNetV2M(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.summary()

# Add custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
outputs = Dense(len(class_names), activation='softmax')(x)

# Combine base model and custom head
model = Model(inputs=base_model.input, outputs=outputs)

# # Define CNN model
# inputs = Input(shape=(224, 224, 3))
# x = Conv2D(32, (3, 3), activation='relu')(inputs)
# x = MaxPooling2D(pool_size=(2, 2))(x)
# x = Conv2D(64, (3, 3), activation='relu')(x)
# x = MaxPooling2D(pool_size=(2, 2))(x)
# x = Conv2D(128, (3, 3), activation='relu')(x)
# x = MaxPooling2D(pool_size=(2, 2))(x)
# x = Flatten()(x)
# x = Dense(256, activation='relu')(x)
# x = BatchNormalization()(x)
# x = Dropout(0.5)(x)
# outputs = Dense(len(class_names), activation='softmax')(x)

# # Create model
# model = Model(inputs=inputs, outputs=outputs)

# # Compile model
# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# # Define ReduceLROnPlateau callback
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

# # Train the model
# history = model.fit(
#     train_ds,
#     validation_data=val_ds,
#     epochs=50,
#     callbacks=[reduce_lr]
# )

# Freeze layers in base model
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define ReduceLROnPlateau callback
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=100,
    callbacks=[reduce_lr]
)

# Clear the session
tf.keras.backend.clear_session()

scores = model.evaluate(test_ds)
scores

history
history.params
history.history.keys()
type(history.history['loss'])
len(history.history['loss'])
history.history['loss'][:5] # show loss for first 5 epochs

# Assuming you have the history object from the model.fit call
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get the number of epochs actually completed
epochs_range = range(len(acc))

plt.figure(figsize=(8, 8))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')

plt.show()

"""Run prediction on a sample image"""

import numpy as np
import matplotlib.pyplot as plt

for images_batch, labels_batch in test_ds.take(1):
    first_image = images_batch[0].numpy().astype('uint8')
    first_label = labels_batch[0].numpy()

    print("First image to predict")
    plt.imshow(first_image)
    plt.show()

    print("Actual label:", class_names[np.argmax(first_label)])

    batch_prediction = model.predict(images_batch)
    print("Predicted label:", class_names[np.argmax(batch_prediction[0])])

"""Write a function for inference"""

def predict(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

from sklearn.metrics import classification_report
import numpy as np

true_labels = []
predicted_labels = []

for images_batch, labels_batch in test_ds:
    true_labels.extend(np.argmax(labels_batch, axis=1))  # Ambil indeks dari label sebenarnya
    batch_prediction = model.predict(images_batch)
    predicted_labels.extend(np.argmax(batch_prediction, axis=1))  # Ambil indeks dari label yang diprediksi

# Buat laporan klasifikasi
class_report = classification_report(true_labels, predicted_labels, target_names=class_names)

print("Classification Report:\n", class_report)

"""convert model"""

tfjs.converters.save_keras_model(model, 'models')

!ls
from google.colab import files

files.download('models')

import tensorflowjs as tfjs
tfjs.converters.save_keras_model(model, 'models')

import shutil
# Lokasi folder yang ingin Anda kompres
folder_path = '/content/models'

# Lokasi tempat menyimpan file ZIP
zip_path = '/content/models.zip'

# Kompres folder menjadi file ZIP
shutil.make_archive('/content/models', 'zip', folder_path)

# Lokasi file ZIP yang ingin diunggah
zip_file = '/content/models.zip'

